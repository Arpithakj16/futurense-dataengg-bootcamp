from pyspark import SparkContext
import findspark
findspark.init()


sc = SparkContext()
rdd= sc.textFile("dataset/modify1_weather.txt")
rdd

rdd1= rdd.map(lambda x: x.split(','))

rdd3=rdd1.map(lambda x: float(x[6]))
rdd3.min()

-7.9

rdd4=rdd1.map(lambda x: float(x[5]))
rdd4.max()

36.0


rdd5=rdd1.map(lambda x: (x[1][5:7],float(x[6])))
grouped_rdd = rdd5.groupByKey()
min_values_rdd = grouped_rdd.mapValues(lambda values: min(values))
min_values_rdd.collect()

[('03', -3.2),
 ('0-', 4.4),
 ('05', 14.3),
 ('06', 0.0),
 ('07', 19.8),
 ('01', -7.9),
 ('02', -3.5),
 ('04', 8.0)]

rdd6=rdd1.map(lambda x: (x[1][5:7],float(x[5])))
grouped_rdd = rdd6.groupByKey()
max_values_rdd = grouped_rdd.mapValues(lambda values: max(values))
max_values_rdd.collect()

[('03', 29.1),
 ('0-', 13.7),
 ('05', 31.1),
 ('06', 33.6),
 ('07', 36.0),
 ('01', 26.5),
 ('02', 26.6),
 ('04', 30.8)]
 



